{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93324150",
        "outputId": "7baaaf9a-f74d-4d80-f13a-d5d1cc9647e4"
      },
      "source": [
        "# Set Groq API Key\n",
        "# The GROQ API key is required to authenticate with the Groq API and use their language models.\n",
        "# We set it as an environment variable, which is a common and secure way to handle API keys.\n",
        "import os\n",
        "\n",
        "# Replace with your actual Groq API key or use Colab secrets\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "print(\"GROQ API Key environment variable set.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROQ API Key environment variable set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e80e95f8",
        "outputId": "5533e89d-255d-4caa-b188-0e86c20aad8e"
      },
      "source": [
        "# Step 2: Define the URL of the webpage to scrape and load the content\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "url = \"https://www.cnbc.com/\" # Replace with the URL you want to scrape\n",
        "\n",
        "try:\n",
        "    loader = WebBaseLoader(url)\n",
        "    web_documents = loader.load()\n",
        "    print(f\"Successfully loaded {len(web_documents)} documents from {url}.\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading content from {url}: {e}\")\n",
        "    web_documents = []\n",
        "    print(\"web_documents list is empty.\")\n",
        ""
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 1 documents from https://www.cnbc.com/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d458934",
        "outputId": "88025d4a-576a-46ba-86cf-914d1dc12ee4"
      },
      "source": [
        "# Step 3: Define the path to the Excel file and load the data\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "\n",
        "excel_file_path = \"/content/AP_Reorganization_Site_Prices.xlsx\" # Replace with the path to your Excel file\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "    print(f\"Successfully loaded data from '{excel_file_path}' into a DataFrame.\")\n",
        "    documents_from_excel = []\n",
        "    for index, row in df.iterrows():\n",
        "        # Convert each row to a string representation and create a Document\n",
        "        page_content = \", \".join([f\"{col}: {value}\" for col, value in row.items()])\n",
        "        metadata = {\"source\": excel_file_path, \"row_index\": index}\n",
        "        documents_from_excel.append(Document(page_content=page_content, metadata=metadata))\n",
        "    print(f\"Converted {len(documents_from_excel)} rows into LangChain Document objects.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{excel_file_path}' was not found.\")\n",
        "    documents_from_excel = []\n",
        "    print(\"documents_from_excel list is empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    documents_from_excel = []\n",
        "    print(\"documents_from_excel list is empty.\")\n",
        "\n",
        ""
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from '/content/AP_Reorganization_Site_Prices.xlsx' into a DataFrame.\n",
            "Converted 4 rows into LangChain Document objects.\n",
            "/content/AP_Reorganization_Site_Prices.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e2b5ec",
        "outputId": "6f94c143-4808-4f5a-edbc-2125b1680c56"
      },
      "source": [
        "# Step 4: Split the documents into texts\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "\n",
        "web_texts = text_splitter.split_documents(web_documents)\n",
        "print(f\"Created {len(web_texts)} text chunks from web documents.\")\n",
        "\n",
        "excel_texts = text_splitter.split_documents(documents_from_excel)\n",
        "print(f\"Created {len(excel_texts)} text chunks from Excel documents.\")\n",
        "\n",
        "# Step 5: Initialize the embeddings model\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "print(\"Initialized HuggingFaceEmbeddings model.\")\n",
        "\n",
        "# Step 6: Create separate vector databases\n",
        "persist_directory_web = \"vector_db_web\"\n",
        "if web_texts:\n",
        "    vectordb_web = Chroma.from_documents(web_texts, embeddings, persist_directory=persist_directory_web)\n",
        "    print(f\"Created Chroma vector database for web texts in '{persist_directory_web}'.\")\n",
        "else:\n",
        "    print(\"web_texts is empty. Cannot create web vector database.\")\n",
        "    vectordb_web = None\n",
        "\n",
        "persist_directory_excel = \"vector_db_excel\"\n",
        "if excel_texts:\n",
        "    vectordb_excel = Chroma.from_documents(excel_texts, embeddings, persist_directory=persist_directory_excel)\n",
        "    print(f\"Created Chroma vector database for Excel texts in '{persist_directory_excel}'.\")\n",
        "else:\n",
        "    print(\"excel_texts is empty. Cannot create Excel vector database.\")\n",
        "    vectordb_excel = None\n",
        "\n",
        "# Step 7: Set up a multi-source retriever\n",
        "retrievers = []\n",
        "if vectordb_web:\n",
        "    retrievers.append(vectordb_web.as_retriever())\n",
        "if vectordb_excel:\n",
        "    retrievers.append(vectordb_excel.as_retriever())\n",
        "\n",
        "multi_source_retriever = None\n",
        "if retrievers:\n",
        "    # Adjust weights if needed based on source importance\n",
        "    weights = [1.0/len(retrievers)] * len(retrievers) # Equal weights\n",
        "    multi_source_retriever = EnsembleRetriever(retrievers=retrievers, weights=weights)\n",
        "    print(\"Multi-source retriever configured using EnsembleRetriever.\")\n",
        "else:\n",
        "    print(\"No vector databases created. Cannot configure multi-source retriever.\")\n",
        "\n",
        "\n",
        "# Step 8: Initialize the language model\n",
        "# Ensure GROQ_API_KEY is set (assuming it's done in a previous cell)\n",
        "llm = ChatGroq(model = \"llama-3.3-70b-versatile\",temperature=0)\n",
        "print(\"Initialized ChatGroq language model.\")\n",
        "\n",
        "# Step 9: Configure RAG chain with the multi-source retriever\n",
        "multi_source_qa_chain = None\n",
        "if multi_source_retriever:\n",
        "    multi_source_qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=multi_source_retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    print(\"RetrievalQA chain set up with multi-source retriever.\")\n",
        "else:\n",
        "    print(\"Multi-source retriever not configured. Cannot set up RAG chain.\")\n",
        "\n",
        "# The multi_source_qa_chain is now ready to be used for querying"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 26 text chunks from web documents.\n",
            "Created 4 text chunks from Excel documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-56-3630092044.py:19: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized HuggingFaceEmbeddings model.\n",
            "Created Chroma vector database for web texts in 'vector_db_web'.\n",
            "Created Chroma vector database for Excel texts in 'vector_db_excel'.\n",
            "Multi-source retriever configured using EnsembleRetriever.\n",
            "Initialized ChatGroq language model.\n",
            "RetrievalQA chain set up with multi-source retriever.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9692497a",
        "outputId": "646d8022-4a70-4878-b9d6-2d85884c62d9"
      },
      "source": [
        "# Step 1: Define a query\n",
        "# Define a query that is relevant to the content of either or both sources.\n",
        "query = \"Market today from CNBC?\"\n",
        "\n",
        "# Step 2 & 3: Invoke the multi_source_qa_chain and store the response\n",
        "# Invoke the RAG chain with the defined query using the multi-source retriever.\n",
        "response = multi_source_qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Step 4: Print the full combined_response\n",
        "# Print the entire output from the RetrievalQA chain.\n",
        "print(\"Full combined response:\")\n",
        "print(response)\n",
        "\n",
        "# Step 5: Print only the generated answer from the combined_response\n",
        "# Extract and print only the 'result' field from the response dictionary.\n",
        "print(\"\\nGenerated answer:\")\n",
        "print(response['result'])\n",
        "\n",
        "# Optional: Step 6: Inspect and print the source documents\n",
        "print(\"\\nSource Documents:\")\n",
        "if 'source_documents' in response and response['source_documents']:\n",
        "    for i, doc in enumerate(response['source_documents']):\n",
        "        print(f\"--- Source Document {i+1} ---\")\n",
        "        print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "        print(f\"Content (first 200 chars): {doc.page_content[:200]}...\")\n",
        "else:\n",
        "    print(\"No source documents found in the response.\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full combined response:\n",
            "{'query': 'Market today from CNBC?', 'result': 'The S&P 500 posted its fifth straight record close this week, powered by solid earnings.', 'source_documents': [Document(id='aa9121b8-fd6c-4dd9-b54e-059462ba1682', metadata={'row_index': 1, 'source': '/content/AP_Reorganization_Site_Prices.xlsx'}, page_content='Site Name: Visakhapatnam, Development Type: IT Hub, Item: Commercial Space (per sqft), Price (INR): 5500'), Document(id='c062af1f-a7cb-4ed4-83a2-d7073530aad5', metadata={'description': 'CNBC International is the world leader for news on business, technology, China, trade, oil prices, the Middle East and markets.', 'title': 'International Business, World News & Global Stock Market Analysis', 'language': 'en', 'source': 'https://www.cnbc.com/'}, page_content=\"that give stock to rank-and-file employees4 Hours AgoDover, Honeywell continue their post-earnings slides. Here's how we may respond4 Hours AgoInvestor Dan Niles' favorite picks for the rest of earnings season4 Hours AgoInflation ahead? Companies will only absorb higher tariff costs for so long4 Hours AgoIntel drops 8% as foundry business axes projects, struggles to find customers4 Hours AgoStocks making the biggest moves midday: Tesla, Intel, Deckers, Charter and more5 Hours AgoHow to trade\"), Document(id='4b07ebf5-48f4-4c90-a1cc-152b603a10a6', metadata={'description': 'CNBC International is the world leader for news on business, technology, China, trade, oil prices, the Middle East and markets.', 'source': 'https://www.cnbc.com/', 'title': 'International Business, World News & Global Stock Market Analysis', 'language': 'en'}, page_content='Data is a real-time snapshot *Data is delayed at least 15 minutes.\\n      Global Business and Financial News, Stock Quotes, and Market Data\\n      and Analysis.\\n    Market Data Terms of Use and DisclaimersData also provided by'), Document(id='76f795dd-46bd-4ab7-abe6-1d71a187a67f', metadata={'language': 'en', 'source': 'https://www.cnbc.com/', 'title': 'International Business, World News & Global Stock Market Analysis', 'description': 'CNBC International is the world leader for news on business, technology, China, trade, oil prices, the Middle East and markets.'}, page_content='ColumnsEducationSubscribeSign InPROPro NewsLivestreamFull EpisodesStock ScreenerMarket ForecastOptions InvestingChart InvestingStock ListsSubscribeSign InLivestreamMenuMake ItselectUSAINTLLivestreamSearch quotes, news & videosLivestreamWatchlistSIGN INMarketsBusinessInvestingTechPoliticsVideoWatchlistInvesting ClubPROLivestreamMenuCreate your own markets bannerUSEURASIAPRE-MKTBONDSFXCRYPTOGOLDOILS&P 500 posts fifth straight record close this week, powered by solid earningsQuick LinksQuick'), Document(id='c649dc13-f073-433a-b180-2506c33581ce', metadata={'description': 'CNBC International is the world leader for news on business, technology, China, trade, oil prices, the Middle East and markets.', 'language': 'en', 'source': 'https://www.cnbc.com/', 'title': 'International Business, World News & Global Stock Market Analysis'}, page_content=\"a challenge1 Hour AgoWhat the prediction markets are saying about the big Wall Street events ahead1 Hour AgoMusk’s Grok AI is now on Kalshi and Polymarket. What could go wrong?2 Hours AgoEarnings season has required two hands on the wheel, and it's not slowing down yet2 Hours AgoPalantir among 20 most valuable U.S. companies, with stock more than doubling in 20252 Hours AgoThe breakout in this utilities ETF has room to run, says Carter Worth2 Hours AgoInside Tesla's new retro-futuristic\")]}\n",
            "\n",
            "Generated answer:\n",
            "The S&P 500 posted its fifth straight record close this week, powered by solid earnings.\n",
            "\n",
            "Source Documents:\n",
            "--- Source Document 1 ---\n",
            "Source: /content/AP_Reorganization_Site_Prices.xlsx\n",
            "Content (first 200 chars): Site Name: Visakhapatnam, Development Type: IT Hub, Item: Commercial Space (per sqft), Price (INR): 5500...\n",
            "--- Source Document 2 ---\n",
            "Source: https://www.cnbc.com/\n",
            "Content (first 200 chars): that give stock to rank-and-file employees4 Hours AgoDover, Honeywell continue their post-earnings slides. Here's how we may respond4 Hours AgoInvestor Dan Niles' favorite picks for the rest of earnin...\n",
            "--- Source Document 3 ---\n",
            "Source: https://www.cnbc.com/\n",
            "Content (first 200 chars): Data is a real-time snapshot *Data is delayed at least 15 minutes.\n",
            "      Global Business and Financial News, Stock Quotes, and Market Data\n",
            "      and Analysis.\n",
            "    Market Data Terms of Use and Disclaim...\n",
            "--- Source Document 4 ---\n",
            "Source: https://www.cnbc.com/\n",
            "Content (first 200 chars): ColumnsEducationSubscribeSign InPROPro NewsLivestreamFull EpisodesStock ScreenerMarket ForecastOptions InvestingChart InvestingStock ListsSubscribeSign InLivestreamMenuMake ItselectUSAINTLLivestreamSe...\n",
            "--- Source Document 5 ---\n",
            "Source: https://www.cnbc.com/\n",
            "Content (first 200 chars): a challenge1 Hour AgoWhat the prediction markets are saying about the big Wall Street events ahead1 Hour AgoMusk’s Grok AI is now on Kalshi and Polymarket. What could go wrong?2 Hours AgoEarnings seas...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wm_xEkJuPuL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b64eef3e",
        "outputId": "d333aaa4-e3c3-466d-a7e6-ec19b51f0886"
      },
      "source": [
        "# Verify the loader object\n",
        "print(\"Loader object:\")\n",
        "print(loader)\n",
        "print(f\"Type of loader: {type(loader)}\")\n",
        "\n",
        "# Verify the content of excel\n",
        "print(\"\\nContent of excel documents:\")\n",
        "if documents_from_excel:\n",
        "    for i, doc in enumerate(documents_from_excel):\n",
        "        print(f\"--- Document {i+1} ---\")\n",
        "        print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print(f\"Page Content (first 500 chars):\\n{doc.page_content[:500]}...\")\n",
        "else:\n",
        "    print(\"documents_from_excel list is empty.\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader object:\n",
            "<langchain_community.document_loaders.web_base.WebBaseLoader object at 0x78a12c752610>\n",
            "Type of loader: <class 'langchain_community.document_loaders.web_base.WebBaseLoader'>\n",
            "\n",
            "Content of excel documents:\n",
            "--- Document 1 ---\n",
            "Source: /content/AP_Reorganization_Site_Prices.xlsx\n",
            "Metadata: {'source': '/content/AP_Reorganization_Site_Prices.xlsx', 'row_index': 0}\n",
            "Page Content (first 500 chars):\n",
            "Site Name: Amaravati, Development Type: Administrative, Item: Land (per acre), Price (INR): 35000000...\n",
            "--- Document 2 ---\n",
            "Source: /content/AP_Reorganization_Site_Prices.xlsx\n",
            "Metadata: {'source': '/content/AP_Reorganization_Site_Prices.xlsx', 'row_index': 1}\n",
            "Page Content (first 500 chars):\n",
            "Site Name: Visakhapatnam, Development Type: IT Hub, Item: Commercial Space (per sqft), Price (INR): 5500...\n",
            "--- Document 3 ---\n",
            "Source: /content/AP_Reorganization_Site_Prices.xlsx\n",
            "Metadata: {'source': '/content/AP_Reorganization_Site_Prices.xlsx', 'row_index': 2}\n",
            "Page Content (first 500 chars):\n",
            "Site Name: Kurnool, Development Type: Judicial Capital, Item: Office Space (per sqft), Price (INR): 4200...\n",
            "--- Document 4 ---\n",
            "Source: /content/AP_Reorganization_Site_Prices.xlsx\n",
            "Metadata: {'source': '/content/AP_Reorganization_Site_Prices.xlsx', 'row_index': 3}\n",
            "Page Content (first 500 chars):\n",
            "Site Name: Anantapur, Development Type: Industrial Zone, Item: Industrial Plot (per acre), Price (INR): 28000000...\n"
          ]
        }
      ]
    }
  ]
}